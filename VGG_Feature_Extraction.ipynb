{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import shutil\n",
    "import os\n",
    "import pandas as pd\n",
    "import matplotlib\n",
    "matplotlib.use(u'nbAgg')\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import pickle\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn import preprocessing\n",
    "import pandas as pd\n",
    "from multiprocessing import Process\n",
    "import multiprocessing\n",
    "import librosa\n",
    "import IPython.display as ipd\n",
    "import librosa.display\n",
    "import sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_sort=np.load('data_sort.npy') #ordered based on audio names. eg: 2,10,15,etc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "label=np.load('label.npy',allow_pickle=True) #ordered based on audio names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename_split_order=np.load('filename_split_order.npy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Split: (MFCC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#split in the same way as Other_Features.ipynb. random_state should be same in both the notebooks\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(data_sort, label, test_size=0.20, random_state=42,stratify=label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Pop', 'Instrumental', 'Experimental', 'Electronic', 'Electronic',\n",
       "       'Instrumental', 'Rock', 'Hip-Hop', 'Experimental', 'Experimental'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " y_train[0:10] #verify this with Other_features.ipynb. y_train and y_test in both the notebooks should be same."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Rock', 'International', 'Folk', 'Folk', 'Electronic', 'Hip-Hop',\n",
       "       'Hip-Hop', 'Pop', 'Instrumental', 'International'], dtype=object)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test[0:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pad with zeros to get minimum size needed to use VGG model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Array of size 6397*20*1293 is converted to 6397*32*1293\n",
    "X_train_32=np.zeros([6397,32,1293])\n",
    "for i in range(len(X_train)):\n",
    "    for j in range(len(X_train[i])):\n",
    "        X_train_32[i,j]=X_train[i,j]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Array of size 1600*20*1293 is converted to 1600*32*1293\n",
    "X_test_32=np.zeros([1600,32,1293])\n",
    "for i in range(len(X_test)):\n",
    "    for j in range(len(X_test[i])):\n",
    "        X_test_32[i,j]=X_test[i,j]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### VGG16 model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /opt/conda/lib/python3.7/site-packages/tensorflow_core/python/ops/resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "If using Keras pass *_constraint arguments to layers.\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.applications import VGG16\n",
    "from tensorflow.keras.layers import Input,Concatenate\n",
    "img_input = Input(shape=(32,1293,1))\n",
    "img_conc = Concatenate()([img_input, img_input, img_input]) #concatenate gray scale image 3 times to represent in 3d.\n",
    "vgg_conv = VGG16(weights='imagenet', include_top=False, input_tensor=img_conc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "vgg16 (Model)                (None, 1, 40, 512)        14714688  \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 20480)             0         \n",
      "=================================================================\n",
      "Total params: 14,714,688\n",
      "Trainable params: 14,714,688\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#https://www.learnopencv.com/keras-tutorial-fine-tuning-using-pre-trained-models/\n",
    "from tensorflow.keras import models\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import optimizers\n",
    "from tensorflow.keras.layers import Flatten\n",
    "\n",
    "\n",
    "model = models.Sequential()\n",
    "model.add(vgg_conv)\n",
    "model.add(Flatten())\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6397, 20480)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Train_Latent=model.predict(X_train_32.reshape(6397,32,1293,1))\n",
    "Train_Latent.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1600, 20480)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Test_Latent=model.predict(X_test_32.reshape(1600,32,1293,1))\n",
    "Test_Latent.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "vgg_features_split_order=np.vstack((Train_Latent,Test_Latent))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('vgg_features_split_order',vgg_features_split_order)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### VGG_Spectrogram:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#input shape is different here compared to MFCC\n",
    "from tensorflow.keras.applications import VGG16\n",
    "from tensorflow.keras.layers import Input\n",
    "\n",
    "img_input = Input(shape=(480,640,3))\n",
    "vgg_conv = VGG16(weights='imagenet', include_top=False, input_tensor=img_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "vgg16 (Model)                (None, 15, 20, 512)       14714688  \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 5, 10, 512)        0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 25600)             0         \n",
      "=================================================================\n",
      "Total params: 14,714,688\n",
      "Trainable params: 14,714,688\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#https://www.learnopencv.com/keras-tutorial-fine-tuning-using-pre-trained-models/\n",
    "from tensorflow.keras import models\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import optimizers\n",
    "from tensorflow.keras.layers import Flatten,Conv2D,MaxPooling2D\n",
    "\n",
    "\n",
    "model = models.Sequential()\n",
    "model.add(vgg_conv)\n",
    "model.add(MaxPooling2D(pool_size=(3, 2))) \n",
    "model.add(Flatten())\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7997/7997 [06:01<00:00, 22.14it/s]\n"
     ]
    }
   ],
   "source": [
    "# load each image as vector and extract its corresponding vgg features \n",
    "#image saved in order of the name of audio(eg: '02.mp3' followed by '10.mp3' and so on) \n",
    "#and order is the name of spectrogram image (eg: '1.png' is the spectrogram of audio '02.mp3')\n",
    "import cv2 \n",
    "from tqdm import tqdm\n",
    "\n",
    "vgg_spectrogram=np.zeros((7997,25600))\n",
    "for i in tqdm(range(7997)): \n",
    "    img=cv2.imread('MFCC_SPECTROGRAM/'+str(i)+'.png')\n",
    "    vgg_spectrogram[i]=model.predict(img.reshape(1,480,640,3))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Split:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#split in the same way as before. random_state should be same in all the notebooks.\n",
    "#splitting is done to rearrange the data points.\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(vgg_spectrogram, label, test_size=0.20, random_state=42,stratify=label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Pop', 'Instrumental', 'Experimental', 'Electronic', 'Electronic',\n",
       "       'Instrumental', 'Rock', 'Hip-Hop', 'Experimental', 'Experimental'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " y_train[0:10] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Rock', 'International', 'Folk', 'Folk', 'Electronic', 'Hip-Hop',\n",
       "       'Hip-Hop', 'Pop', 'Instrumental', 'International'], dtype=object)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "vgg_spectrogram_features_split_order=np.vstack((X_train,X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7997, 25600)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vgg_spectrogram_features_split_order.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('vgg_spectrogram_features_split_order',vgg_spectrogram_features_split_order)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### VGG_Chromagram:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /opt/conda/lib/python3.7/site-packages/tensorflow_core/python/ops/resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "If using Keras pass *_constraint arguments to layers.\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.applications import VGG16\n",
    "from tensorflow.keras.layers import Input\n",
    "\n",
    "img_input = Input(shape=(480,640,3))\n",
    "vgg_conv = VGG16(weights='imagenet', include_top=False, input_tensor=img_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "vgg16 (Model)                (None, 15, 20, 512)       14714688  \n",
      "_________________________________________________________________\n",
      "max_pooling2d (MaxPooling2D) (None, 5, 10, 512)        0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 25600)             0         \n",
      "=================================================================\n",
      "Total params: 14,714,688\n",
      "Trainable params: 14,714,688\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#https://www.learnopencv.com/keras-tutorial-fine-tuning-using-pre-trained-models/\n",
    "from tensorflow.keras import models\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import optimizers\n",
    "from tensorflow.keras.layers import Flatten,Conv2D,MaxPooling2D\n",
    "\n",
    "\n",
    "model = models.Sequential()\n",
    "model.add(vgg_conv)\n",
    "model.add(MaxPooling2D(pool_size=(3, 2))) \n",
    "model.add(Flatten())\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7997/7997 [05:57<00:00, 22.35it/s]\n"
     ]
    }
   ],
   "source": [
    "# load each image as vector and extract its corresponding vgg features \n",
    "# and the order is same as train-test split\n",
    "import cv2 \n",
    "from tqdm import tqdm\n",
    "\n",
    "vgg_chromagram=np.zeros((7997,25600))\n",
    "for e,i in enumerate(tqdm(filename_split_order)): \n",
    "    img=cv2.imread('Chroma_Spectrogram/'+i+'.png')\n",
    "    vgg_chromagram[e]=model.predict(img.reshape(1,480,640,3))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('vgg_chromagram_split_order',vgg_chromagram)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### VGG_Mel_Spectrogram:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /opt/conda/lib/python3.7/site-packages/tensorflow_core/python/ops/resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "If using Keras pass *_constraint arguments to layers.\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.applications import VGG16\n",
    "from tensorflow.keras.layers import Input\n",
    "\n",
    "img_input = Input(shape=(480,640,3))\n",
    "vgg_conv = VGG16(weights='imagenet', include_top=False, input_tensor=img_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "vgg16 (Model)                (None, 15, 20, 512)       14714688  \n",
      "_________________________________________________________________\n",
      "max_pooling2d (MaxPooling2D) (None, 5, 10, 512)        0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 25600)             0         \n",
      "=================================================================\n",
      "Total params: 14,714,688\n",
      "Trainable params: 14,714,688\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#https://www.learnopencv.com/keras-tutorial-fine-tuning-using-pre-trained-models/\n",
    "from tensorflow.keras import models\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import optimizers\n",
    "from tensorflow.keras.layers import Flatten,Conv2D,MaxPooling2D\n",
    "\n",
    "\n",
    "model = models.Sequential()\n",
    "model.add(vgg_conv)\n",
    "model.add(MaxPooling2D(pool_size=(3, 2))) \n",
    "model.add(Flatten())\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7997/7997 [07:46<00:00, 17.13it/s]\n"
     ]
    }
   ],
   "source": [
    "# load each image as vector and extract its corresponding vgg features \n",
    "# and the order is same as train-test split\n",
    "import cv2 \n",
    "from tqdm import tqdm\n",
    "\n",
    "vgg_Mel_Spectrogram=np.zeros((7997,25600))\n",
    "for e,i in enumerate(tqdm(filename_split_order)): \n",
    "    img=cv2.imread('Mel_Spectrogram/'+i+'.png')\n",
    "    vgg_Mel_Spectrogram[e]=model.predict(img.reshape(1,480,640,3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('vgg_mel_spectrogram_split_order',vgg_Mel_Spectrogram)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### VGG_Mel_Spectrogram (unflattened):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /opt/conda/lib/python3.7/site-packages/tensorflow_core/python/ops/resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "If using Keras pass *_constraint arguments to layers.\n"
     ]
    }
   ],
   "source": [
    "#Load the VGG16 model and remove final layer(ie:'include_top=False')\n",
    "from tensorflow.keras.applications import VGG16\n",
    "from tensorflow.keras.layers import Input\n",
    "\n",
    "img_input = Input(shape=(480,640,3))\n",
    "vgg_conv = VGG16(weights='imagenet', include_top=False, input_tensor=img_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "vgg16 (Model)                (None, 15, 20, 512)       14714688  \n",
      "_________________________________________________________________\n",
      "max_pooling2d (MaxPooling2D) (None, 7, 10, 512)        0         \n",
      "=================================================================\n",
      "Total params: 14,714,688\n",
      "Trainable params: 14,714,688\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#https://www.learnopencv.com/keras-tutorial-fine-tuning-using-pre-trained-models/\n",
    "from tensorflow.keras import models\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import optimizers\n",
    "from tensorflow.keras.layers import Flatten,Conv2D,MaxPooling2D\n",
    "\n",
    "\n",
    "model = models.Sequential()\n",
    "model.add(vgg_conv)\n",
    "model.add(MaxPooling2D(pool_size=(2, 2))) #different from flattened model which has (3,2)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7997/7997 [08:09<00:00, 16.34it/s]\n"
     ]
    }
   ],
   "source": [
    "# load each image as vector and extract its corresponding vgg features \n",
    "# and the order is same as train-test split\n",
    "import cv2 \n",
    "from tqdm import tqdm\n",
    "\n",
    "vgg_Mel_Spectrogram_unflat=np.zeros((7997,7,10,512))\n",
    "for e,i in enumerate(tqdm(filename_split_order)): \n",
    "    img=cv2.imread('Mel_Spectrogram/'+i+'.png')\n",
    "    vgg_Mel_Spectrogram_unflat[e]=model.predict(img.reshape(1,480,640,3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('vgg_Mel_Spectrogram_unflat_split_order',vgg_Mel_Spectrogram_unflat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
